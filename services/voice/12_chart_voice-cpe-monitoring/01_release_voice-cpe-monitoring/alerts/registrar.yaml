apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  annotations:
    meta.helm.sh/release-name: register-rules
    meta.helm.sh/release-namespace: voice
  name: register-rules
  namespace: voice
spec:
  groups:
    - name: voice-registrar
      rules:
      - alert: kafka_unavailable
        expr: kafka_producer_state{pod=~"voice-registrar.*"} == 0 or kafka_consumer_state{pod=~"voice-registrar.*"} == 0
        for: 5m
        labels:
          severity: critical
          category: voice_pager
          service: voice
          servicename: voice-registrar
        annotations:
          description: Kafka is not available for pod {{ $labels.pod }}
          runbook: If alarm is triggered for multiple services make sure there is no issues with Kafka, restart Kafka. If alarm is triggered only for pod {{ $labels.pod }} check if there is any issue with the pod
      - alert: kafka_latency
        expr: histogram_quantile(0.95, sum(rate(kafka_consumer_latency_bucket{topic="voice-registrar"}[1m])) by (le)) > 1
        for: 5m
        labels:
          severity: warning
          service: voice
          servicename: voice-registrar
        annotations:
          description: Latency for more than 5% of messages within 1 minute observation period is more than 1 s for topic {{ $labels.topic }} consistenly for 5 minutes
          runbook: If alarm is triggered for multiple topics make sure there is no issues with Kafka (CPU, memory or network overload). If alarm is triggered only for topic {{ $labels.topic }} check if there is any issue with the service related to the topic (CPU, memory or network overload)
      - alert: kafka_consumer_health_checks
        expr: increase(kafka_consumer_error_total{topic="voice-registrar", error="healthcheck"}[5m]) > 10
        labels:
          severity: warning
          service: voice
          servicename: voice-registrar
        annotations:
          description: Health check failed more than 10 times in 5 min for Kafka consumer for topic {{$labels.topic}}
          runbook: If alarm is triggered for multiple services make sure there is no issues with Kafka, restart Kafka. If alarm is triggered only for {{ $labels.container }}, check if there is any issue with the service
      - alert: kafka_consumer_timeouts
        expr: increase(kafka_consumer_error_total{topic="voice-registrar", error="timeout"}[5m]) > 10
        labels:
          severity: warning
          service: voice
          servicename: voice-registrar
        annotations:
          description: More than 10 request timeouts appeared in 5 min for Kafka consumer for topic {{$labels.topic}}
          runbook: If alarm is triggered for multiple services make sure there is no issues with Kafka, restart Kafka. If alarm is triggered only for {{ $labels.container }}, check if there is any issue with the service
      - alert: kafka_consumer_crashes
        expr: increase(kafka_consumer_error_total{topic="voice-registrar",error="crash"}[5m]) > 3
        labels:
          severity: critical
          category: voice_pager
          service: voice
          servicename: voice-registrar
        annotations:
          description: More than 3 Kafka consumer crashes in 5 min for service {{ $labels.container }}
          runbook: If alarm is triggered for multiple services make sure there is no issues with Kafka, restart Kafka. If alarm is triggered only for {{ $labels.container }}, check if there is any issue with the service
      - alert: RedisDownFor5min
        expr: registrar_redis_state != 2
        for: 5m
        labels:
          severity: warning
          service: voice
          servicename: voice-registrar
        annotations:
            description: "Redis is not available for pod {{ $labels.pod }} for 5 min {{ $labels.pod }}"
            summary: "Registrar node pod is not connected to redis 5 mins"
      - alert: RedisDownFor10min
        expr: registrar_redis_state != 2
        for: 10m
        labels:
          severity: critical
          category: voice_pager
          service: voice
          servicename: voice-registrar
        annotations:
            description: "Redis is not available for pod {{ $labels.pod }} for 10 min {{ $labels.pod }}"
            summary: "Registrar node pod is not connected to redis 10 mins"
      - alert: registrar_node_memory_usage
        expr: (sum by(pod) (container_memory_working_set_bytes{pod=~"voice-registrar.*",image!=""}) / sum by(pod)(kube_pod_container_resource_limits{pod=~"voice-registrar.*",unit="byte"})) * 100 > 65
        for: 5m
        labels:
          severity: warning
          service: voice
          servicename: voice-registrar
        annotations:
            description: "The registrar node container memeory crossed 65% for 5 minutes for {{ $labels.pod }}"
            summary: "The registrar node container memeory crossed 65% for 5 minute"
      - alert: registrar_node_memory_usage_80
        expr: (sum by(pod) (container_memory_working_set_bytes{pod=~"voice-registrar.*",image!=""}) / sum by(pod)(kube_pod_container_resource_limits{pod=~"voice-registrar.*",unit="byte"})) * 100 > 80
        for: 5m
        labels:
          severity: critical
          category: voice_pager
          service: voice
          servicename: voice-registrar
        annotations:
            description: "The registrar node container memeory crossed 80% for 5 minutes for {{ $labels.pod }}"
            summary: "The registrar node container memeory crossed 80% for 5 minute"
      - alert: registrar_node_cpu_usage
        expr: (sum by(pod) (rate(container_cpu_usage_seconds_total{pod=~"voice-registrar.*", image!=""}[5m])) /( sum by(pod)(kube_pod_container_resource_limits{pod=~"voice-registrar.*",unit="core"}))) * 100 > 65
        for: 5m
        labels:
          severity: warning
          service: voice
          servicename: voice-registrar
        annotations:
            description: "The registrar node container cpu crossed 65% for 5 minutes for {{ $labels.pod }}"
            summary: "The registrar node container cpu crossed 65% for 5 minute"
      - alert: registrar_node_cpu_usage_80
        expr: (sum by(pod) (rate(container_cpu_usage_seconds_total{pod=~"voice-registrar.*", image!=""}[5m])) /( sum by(pod)(kube_pod_container_resource_limits{pod=~"voice-registrar.*",unit="core"}))) * 100 > 80
        for: 5m
        labels:
          severity: critical
          category: voice_pager
          service: voice
          servicename: voice-registrar
        annotations:
            description: "The registrar node container cpu crossed 80% for 5 minutes for {{ $labels.pod }}"
            summary: "The registrar node container cpu crossed 80% for 5 minute"
      - alert: PodStatusFailed
        expr: kube_pod_status_phase{pod=~"voice-registrar.*",pod!~"voice-registrar-test-.*",phase="Failed"} == 1
        labels:
          severity: warning
          service: voice
          category: voice_pager
          servicename: voice-registrar
        annotations:
            description: "The registrar node pod went to failed state for {{ $labels.pod }}"
            summary: "The registrar node pod went to failed state"
      - alert: PodStatusUnknown
        expr: kube_pod_status_phase{pod=~"voice-registrar.*",pod!~"voice-registrar-test-.*",phase="Unknown"} == 1
        for: 5m
        labels:
          severity: warning
          service: voice
          category: voice_pager
          servicename: voice-registrar
        annotations:
            description: "The registrar node pod went to Unknown state for 5min for {{ $labels.pod }}"
            summary: "The registrar node pod went to Unknown state for 5min"
      - alert: PodStatusPending
        expr: kube_pod_status_phase{pod=~"voice-registrar.*",pod!~"voice-registrar-test-.*",phase="Pending"} == 1
        for: 5m
        labels:
          severity: warning
          service: voice
          category: voice_pager
          servicename: voice-registrar
        annotations:
            description: "The registrar node pod went to Pending state for 5min for {{ $labels.pod }}"
            summary: "The registrar node pod went to Pending state for 5min"
      - alert: PodStatusNotReadyfor10mins
        expr: kube_pod_status_ready{pod=~"voice-registrar.*",pod!~"voice-registrar-test-.*",condition="true"} != 1
        for: 10m
        labels:
          severity: critical
          category: voice_pager
          service: voice
          servicename: voice-registrar
        annotations:
            description: "The registrar node pod went to Not ready status for 10 mins for {{ $labels.pod }}"
            summary: "The registrar node pod went to Not ready status for 10 mins"
      - alert: ContainerRestartedRepeatedly
        expr: increase(kube_pod_container_status_restarts_total{container="voice-registrar"}[15m]) >= 5
        labels:
          severity: critical
          category: voice_pager
          service: voice
          servicename: voice-registrar
        annotations:
          description: "The registrar node container was restarted 5 or more times within 15 mins for {{ $labels.pod }}"
          summary: "The registrar node container was restarted 5 or more times within 15 mins"
      - alert: pod_sidecars_failed
        expr: count(container_memory_usage_bytes{pod=~"voice-registrar.*",pod!~"voice-.+-test-.*"}) by (pod) < 5
        for: 10m
        labels:
          severity: critical
          category: voice_pager
          service: voice
          servicename: voice-registrar
        annotations:
          description: Pod sidecar failed to start {{ $labels.pod }}
          runbook: Check if there is any problem with consul {{ $labels.pod }}, restart pod
      - alert: aggregated_service_health_failed
        expr: registrar_health_level < 2
        for: 5m
        labels:
          severity: critical
          category: voice_pager
          service: voice
          servicename: voice-registrar
        annotations:
          description: Dependent services (redis/kafka/config node) or envoy sidecar is not available for 5 mins in pod {{ $labels.pod }}
          runbook: Check registrar dashboard for Aggregated Service Health Errors panel. For redis error, check issues/crash in pod, otherwise restart redis. For envoy error, registrar container will be restarted by liveness probe and if still issue exists, restart pod
      - alert: config_node_fail
        expr: increase(http_client_response_count{container="voice-registrar",status!~"2XX"}[5m]) > 1
        labels:
          severity: warning
          service: voice
          servicename: voice-registrar
        annotations:
          description: Request to config node failed
          runbook: Check if there is any problem with pod {{ $labels.pod }} and config node
      - alert: log_messages_rate_exceeds_limit
        expr: sum(rate(log_output_bytes_total{container="voice-registrar"}[1m])) by(container) > (1.65 * 1000 * 1000)
        for: 1m
        labels:
          severity: warning
          service: voice
          servicename: voice-registrar
        annotations:
          description: Container {{ $labels.container }} Log messages rate exceeds HLA requirement (100 MB/min or 1.65 MB/s)
          runbook: Contact service team to reduce log rate
