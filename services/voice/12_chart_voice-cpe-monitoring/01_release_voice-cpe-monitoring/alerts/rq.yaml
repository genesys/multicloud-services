apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  annotations:
    meta.helm.sh/release-name: rq-rules
    meta.helm.sh/release-namespace: voice
  name: rq-rules
  namespace: voice
spec: 
  groups:
    - name: voice-rq
      rules: 
      - alert: rq_node_number_of_streams_too_high 
        expr: rqnode_streams{pod=~"voice-rq.*"} > 100000
        for: 5m
        labels:
          severity: critical
          category: voice_pager
          service: voice
          servicename: voice-rq
        annotations:
            description: "The trigger will flag an alarm when numbr of rq streams exceed 10000"
            summary: "rq node has more then 10000 opened streams"
      - alert: rq_node_redis_down_for_5min
        expr: rqnode_redis_state{pod=~"voice-rq.*"} != 2
        for: 5m
        labels:
          severity: warning
          service: voice
          servicename: voice-rq
        annotations:
            description: "The trigger will flag an alarm when rq node is not connected to redis 5 mins"
            summary: "rq node pod is not connected to redis 5 mins"
      - alert: rq_node_redis_down_for_10min
        expr: rqnode_redis_state{pod=~"voice-rq.*"} != 2
        for: 10m
        labels:
          severity: critical
          category: voice_pager
          service: voice
          servicename: voice-rq
        annotations:
            description: "The trigger will flag an alarm when rq node is not connected to redis 10 mins"
            summary: "rq node pod is not connected to redis 10 mins"

      - alert: rq_node_memory_usage
        expr: (sum by(pod) (container_memory_working_set_bytes{container="voice-rq"}) / sum by(pod)(kube_pod_container_resource_limits{container="voice-rq",unit="byte"})) * 100 > 65
        for: 5m
        labels:
          severity: warning
          service: voice
          servicename: voice-rq
        annotations:
            description: "The rq node container memeory crossed 65% for 5 minutes"
            summary: "The rq node container memeory crossed 65% for 5 minute"
      - alert: rq_node_memory_usage_80
        expr: (sum by(pod) (container_memory_working_set_bytes{container="voice-rq"}) / sum by(pod)(kube_pod_container_resource_limits{container="voice-rq",unit="byte"})) * 100 > 80
        for: 5m
        labels:
          severity: critical
          category: voice_pager
          service: voice
          servicename: voice-rq
        annotations:
            description: "The rq node container memeory crossed 80% for 5 minutes"
            summary: "The rq node container memeory crossed 80% for 5 minute"
      - alert: rq_node_cpu_usage
        expr: (sum by(pod) (rate(container_cpu_usage_seconds_total{pod=~"voice-rq.*", image!=""}[5m])) /( sum by(pod)(kube_pod_container_resource_limits{pod=~"voice-rq.*",unit="core"}))) * 100 > 65
        for: 5m
        labels:
          severity: warning
          service: voice
          servicename: voice-rq
        annotations:
            description: "The rq node container cpu crossed 65% for 5 minutes"
            summary: "The rq node container cpu crossed 65% for 5 minute"
      - alert: rq_node_cpu_usage_80
        expr: (sum by(pod) (rate(container_cpu_usage_seconds_total{pod=~"voice-rq.*", image!=""}[5m])) /( sum by(pod)(kube_pod_container_resource_limits{pod=~"voice-rq.*",unit="core"}))) * 100 > 80
        for: 5m
        labels:
          severity: critical
          category: voice_pager
          service: voice
          servicename: voice-rq
        annotations:
            description: "The rq node container cpu crossed 80% for 5 minutes"
            summary: "The rq node container cpu crossed 80% for 5 minute"
      - alert: PodStatusFailed
        expr: kube_pod_status_phase{pod=~"voice-rq.*",pod!~"voice-rq-test-.*",phase="Failed"} == 1
        labels:
          severity: warning
          service: voice
          servicename: voice-rq
        annotations:
            description: "The rq node pod went to failed state"
            summary: "The rq node pod went to failed state"
      - alert: PodStatusUnknown
        expr: kube_pod_status_phase{pod=~"voice-rq.*",pod!~"voice-rq-test-.*",phase="Unknown"} == 1
        for: 5m
        labels:
          severity: warning
          service: voice
          servicename: voice-rq
        annotations:
            description: "The rq node pod went to Unknown state for 5min"
            summary: "The rq node pod went to Unknown state for 5min"
      - alert: PodStatusPending
        expr: kube_pod_status_phase{pod=~"voice-rq.*",phase="Pending"} == 1
        for: 5m
        labels:
          severity: warning
          service: voice
          servicename: voice-rq
        annotations:
            description: "The rq node pod went to Pending state for 5min"
            summary: "The rq node pod went to Pending state for 5min"
      - alert: PodStatusNotReadyfor10mins
        expr: kube_pod_status_ready{pod=~"voice-rq.*",pod!~"voice-rq-test-.*",condition="true"} != 1
        for: 10m
        labels:
          severity: critical
          category: voice_pager
          service: voice
          servicename: voice-rq
        annotations:
            description: "The rq node pod went to Not ready status for 10 mins"
            summary: "The rq node pod went to Not ready status for 10 mins"
      - alert: ContainerRestartedRepeatedly
        expr: increase(kube_pod_container_status_restarts_total{container="voice-rq"}[15m]) >= 5
        labels:
          severity: critical
          category: voice_pager
          service: voice
          servicename: voice-rq
        annotations:
          description: "The rq node container was restarted 5 or more times within 15 mins"
          summary: "The rq node container was restarted 5 or more times within 15 mins"
      - alert: pod_sidecars_failed
        expr: count(container_memory_usage_bytes{pod=~"voice-rq.*",pod!~"voice-.+-test-.*"}) by (pod) < 5
        for: 10m
        labels:
          severity: critical
          category: voice_pager
          service: voice
          servicename: voice-rq
        annotations:
          description: Pod sidecar failed to start {{ $labels.pod }}
          runbook: Check if there is any problem with consul {{ $labels.pod }}, restart pod
      - alert: log_messages_rate_exceeds_limit
        expr: sum(rate(log_output_bytes_total{container="voice-rq"}[1m])) by(container) > (1.65 * 1000 * 1000)
        for: 1m
        labels:
          severity: warning
          service: voice
          servicename: voice-rq
        annotations:
          description: Container {{ $labels.container }} Log messages rate exceeds HLA requirement (100 MB/min or 1.65 MB/s)
          runbook: Contact service team to reduce log rate