# Docker image configuration
image:
  registry: $IMAGE_REGISTRY/auth
  # Configure imagePullSecrets if docker registry requires authentication
  # https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/#registry-secret-existing-credentials
  imagePullSecrets:
    - name: pullsecret
  pullPolicy: IfNotPresent
COLOR: blue

postgres:
  deploy: false
  secret:
    name_override:
    create: true
  configmap:
    name_override:
    create: true
  image: postgres:11-alpine
  # REQUIRED FIELDS:
  username: $gauth_pg_username
  password: $gauth_pg_password
  db: $gauth_pg_dbname
  host: $POSTGRES_ADDR
  port: $POSTGRES_PORT

redis:
  deploy: false
  secret:
    name_override: gauth-redis
    create: true
  configmap:
    name_override:
    create: true
  image: redis:5-stretch
  cluster_nodes: ${REDIS_ADDR}:${REDIS_PORT}
  use_tls: false
  # REDIS AUTH
  password_required: true
  password: $REDIS_PASSWORD

services:
  secrets:
    useSecretProviderClass: false
    aadpodidbinding: gauth-identity
    secretProviderClassNames:
      jks_keyvault: keyvault-gauth-jks-keyvault
      jks_credentials: keyvault-gauth-jks-credentials
      admin_user: keyvault-gauth-admin-user
      pg_user: keyvault-gauth-pg-user
      redis_password: keyvault-gauth-redis-password
      client_credentials: keyvault-gauth-client-credentials
      consul_token: keyvault-consul-consul-gauth-token

  # Number of pod replicas, recommended to be N+1
  replicas: 2
  location: /$LOCATION
  db:
    init: true
    poolSize: 3
    poolCheckoutTimeout: 3000
    ssl: disable
  secret:
    name_override:
    create: true
    # REQUIRED FIELDS:
    admin_username: $gauth_admin_username
    admin_password: $gauth_admin_password
    client_id:      $gauth_gws_client_id
    client_secret:  $gauth_gws_client_secret

  auth_ui:
    # Pod resources requests and limits
    resources:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 100m
        memory: 512Mi

  auth:
    jks:
      enabled: true
      sso:
        enabled: true
      secret:
        create: true

      # Convert keystore.jks file content to base64 ( cat keystore.jks | base64 ) and put the output here
      keyStore: jksStorage.jks

      # Keystore file generation: keytool -keystore gauth_keystore.jks -genkey -alias gws-auth-key -storepass <keyStorePassword> -keypass <keyPassword> -keyalg RSA
      # Validation: keytool -list -v -keystore gauth_keystore.jks
      keyStoreFileData: $gauth_jks_key
      # Keystore password
      keyStorePassword: $gauth_jks_keyStorePassword
      # SSL key alias
      keyAlias: gws-auth-key
      # SSL key password
      keyPassword: $gauth_jks_keyPassword
    env:
      # Will override built-in variable. Short notation is important for GKE with Cluster DNS scope
      GWS_AUTH_COMMON_ENV_SERVICE: http://gauth-environment.${NS}:80
    # Number of pod replicas, recommended to be N+1, defaults to services.replica if empty
    replicas: 1
    # Pod resources requests and limits
    resources:
      requests:
        cpu: 300m
        memory: 1Gi
      limits:
        cpu: "1"
        memory: 3Gi
    # readiness probe.
    # To disable readinessProbe - set readinessProbe: {}
    readinessProbe:
      httpGet:
        path: /health
        port: management
      initialDelaySeconds: 30
      timeoutSeconds: 3
      periodSeconds: 10
    # liveness probe
    # To disable livenessProbe - set livenessProbe: {}
    livenessProbe:
      httpGet:
        path: /health
        port: management
      initialDelaySeconds: 120
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 3
      failureThreshold: 3

  environment:
    env:
      # Will override built-in variable. Short notation is important for GKE with Cluster DNS scope
      GWS_ENVIRONMENT_COMMON_AUTH_SERVICE: http://gauth-auth.${NS}:80
    # Number of pod replicas, recommended to be N+1, defaults to services.replica if empty
    replicas: 1
    force_writable: true
    # Pod resources requests and limits
    resources:
      requests:
        cpu: 300m
        memory: 1Gi
      limits:
        cpu: "1"
        memory: 3Gi
    # readiness probe.
    # To disable readinessProbe - set readinessProbe: {}
    readinessProbe:
      httpGet:
        path: /health
        port: management
      initialDelaySeconds: 30
      timeoutSeconds: 3
      periodSeconds: 10
    # liveness probe
    # To disable livenessProbe - set livenessProbe: {}
    livenessProbe:
      httpGet:
        path: /health
        port: management
      initialDelaySeconds: 120
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 3
      failureThreshold: 3

# Optional Deployment/ReplicaSet parameters
optional:
  # https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  priorityClassName: ''
  #  https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  securityContext:
    runAsNonRoot: true
    runAsUser: 500
    runAsGroup: 500
    fsGroup: 500

  #  https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeSelector: {}
  #  https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                gauth: '{{ .gauth }}'
                app.kubernetes.io/name: '{{ include "auth.name" . }}'
                app.kubernetes.io/instance: '{{ .Release.Name }}'
            topologyKey: failure-domain.beta.kubernetes.io/zone
          weight: 100

# https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 10
      maxUnavailable: 0


# Ingress config
# tps://kubernetes.io/docs/concepts/services-networking/ingress/
ingress:
  enabled: true
  # Host that will be used by ingress.
  # Applies to all inbound traffic through the host specified
  frontend: gauth.$DOMAIN
  annotations:
    # Default annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
    route.openshift.io/termination: edge
  # HTTPS configuration
  tls: {}

# Internal  config
# tps://kubernetes.io/docs/concepts/services-networking/ingress/
internal_ingress:
  enabled: true
  # Host that will be used by ingress.
  # Applies to all inbound traffic through the host specified
  frontend: gauth-int.$DOMAIN
  annotations:
    # Default annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
    route.openshift.io/termination: edge
  # HTTPS configuration
  tls: {}


monitoring:
  # Deploy ServiceMonitor
  enabled: true
  # Metrics Scraping Interval
  interval: 15s
  # Create ConfigMap with Grafana Dashboards
  dashboards: true
  # Create PrometheusRule k8s object with alarm definitions
  alarms: true
  # Create rules with CRITICAL severity for Pager Duty integration.
  # `false` will change severity to HIGH (email notifications)
  pagerduty: true

pod_autoscaler:
  auth:
    enabled: false
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 350
  environment:
    enabled: false
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 350
